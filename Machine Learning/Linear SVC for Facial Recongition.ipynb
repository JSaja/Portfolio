{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification - Olivetti Faces\n",
    "**Author: John Saja**\n",
    "\n",
    "In this notebook, we will explore 2 classification methods on the Olivetti Faces dataset - Linear SVC and Random Forest.\n",
    "Additionally, this notebook presents a third party technology named TPOT that was used to select Linear SVC as the first classification model.\n",
    "\n",
    "## Import dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-12-e23d2e98a37f>:25 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-e23d2e98a37f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0msqlContext\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSQLContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0mspark\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\spark\\spark-2.1.0-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    113\u001b[0m         \"\"\"\n\u001b[1;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32mC:\\spark\\spark-2.1.0-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    270\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    271\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[0;32m--> 272\u001b[0;31m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[1;32m    273\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    274\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-12-e23d2e98a37f>:25 "
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "spark_path = \"C:\\spark\\spark-2.1.0-bin-hadoop2.7\"\n",
    "\n",
    "os.environ['SPARK_HOME'] = spark_path\n",
    "os.environ['HADOOP_HOME'] = spark_path\n",
    "\n",
    "sys.path.append(spark_path + \"/bin\")\n",
    "sys.path.append(spark_path + \"/python\")\n",
    "sys.path.append(spark_path + \"/python/pyspark/\")\n",
    "sys.path.append(spark_path + \"/python/lib\")\n",
    "sys.path.append(spark_path + \"/python/lib/pyspark.zip\")\n",
    "sys.path.append(spark_path + \"/python/lib/py4j-0.10.4-src.zip\")\n",
    "\n",
    "import types\n",
    "import collections\n",
    "\n",
    "import pyspark\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark import SparkConf, SparkContext\n",
    "\n",
    "sc = SparkContext()\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc)\n",
    "\n",
    "import pyspark.sql\n",
    "from pyspark.sql.functions import col, avg\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import UserDefinedFunction\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import StandardScaler\n",
    "from sklearn import tree\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import matplotlib.patches as mpatches\n",
    "import pickle\n",
    "from sklearn import datasets\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our Dataset - Olivetti Faces (sklearn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset consists of 400 faces\n"
     ]
    }
   ],
   "source": [
    "olivettiFaces = datasets.fetch_olivetti_faces(data_home=None, shuffle=False, random_state=0, download_if_missing=True)\n",
    "\n",
    "\n",
    "X = olivettiFaces.data\n",
    "y = olivettiFaces.target\n",
    "\n",
    "numRows, numCols = X.shape\n",
    "\n",
    "#Center faces\n",
    "centered = X - X.mean(axis=0)\n",
    "centered -= centered.mean(axis=1).reshape(numRows, -1)\n",
    "X = centered\n",
    "\n",
    "print(\"Dataset consists of %d faces\" % numRows)\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot a sample of our Training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_shape = (64, 64)\n",
    "\n",
    "n_row, n_col = 5, 10\n",
    "n_components = n_row * n_col\n",
    "\n",
    "def plot_gallery(title, images, n_col=n_col, n_row=n_row):\n",
    "    plt.figure(figsize=(2. * n_col, 2.26 * n_row))\n",
    "    plt.suptitle(title, size=32)\n",
    "    for i, comp in enumerate(images):\n",
    "        plt.subplot(n_row, n_col, i + 1)\n",
    "        vmax = max(comp.max(), -comp.min())\n",
    "        plt.imshow(comp.reshape(image_shape), cmap=plt.cm.gray,\n",
    "                   interpolation='nearest',\n",
    "                   vmin=-vmax, vmax=vmax)\n",
    "        plt.xticks(())\n",
    "        plt.yticks(())\n",
    "    plt.subplots_adjust(0.01, 0.05, 0.99, 0.93, 0.04, 0.)\n",
    "plot_gallery(\"First 5 centered Olivetti faces (10 examples each)\", centered[:n_components])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification method # 1 - Linear SVC (Support Vector Classification)\n",
    "\n",
    "I was persuaded to use this by the results generated from using TPOT on this dataset - a classification tool that takes in raw input data and completely takes care of feature selection, preprocessing, feature construction, model selection, and parameter optimization, as can be seen by the graphic below.\n",
    "\n",
    "\n",
    "![title](https://raw.githubusercontent.com/rhiever/tpot/master/images/tpot-ml-pipeline.png)\n",
    "\n",
    "Source: http://rhiever.github.io/tpot/\n",
    "\n",
    "The following code for the model is directly from the output of the TPOT pipeline. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline code to generate best performing model\n",
    "tpot = TPOTClassifier(generations=1, population_size = 10, verbosity = 2, n_jobs=1)\n",
    "\n",
    "tpot.fit(X_train, y_train)\n",
    "\n",
    "print(tpot.score(X_test, y_test))\n",
    "\n",
    "#Exports model code to python file\n",
    "\n",
    "tpot.export('tpot_exported_pipeline.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Linear SVC Scalability**\n",
    "\n",
    "The documentation for this model states that the time complexity is > O(N^4) for any more than a few 10,000 samples. Thus as it currently stands, this model will scale absolutely *terribly*.\n",
    "\n",
    "In general, one way that SVM Classification scales to big data sizes is through implementations of online algorithms. Though there is no such algorithm implemented in Python that has a better performance than the sklearn library, a close competitor would be the LaSVM, written in C but wrappable in Python. LaSVM uses online approximation and only a single pass through the training set, relying on the Sequential Minimal Optimization solution to the problem of quadratic programming. This means that it spends less time training, but achieves results similar to algorithms in popular libraries such as sklearn.\n",
    "\n",
    "Source: http://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html\n",
    "\n",
    "Source: http://leon.bottou.org/projects/lasvm\n",
    "\n",
    "The code below is what was generated from the code above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\John\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_split.py:581: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of groups for any class cannot be less than n_splits=3.\n",
      "  % (min_groups, self.n_splits)), Warning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5-fold CV accuracy of optimized linearSVC model: 0.68 (+/- 0.21)\n",
      "\n",
      "Generalization score =  97.5 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "svcModel = LinearSVC(C=10.0, dual=True, loss=\"squared_hinge\", penalty=\"l2\", tol=0.0001)\n",
    "\n",
    "svcModel.fit(X_train, y_train)\n",
    "\n",
    "# evaluate accuracy\n",
    "scores = cross_val_score(svcModel, X_test, y_test, scoring='accuracy')\n",
    "print(\"5-fold CV accuracy of optimized linearSVC model: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n",
    "#Estimate test error\n",
    "pred = svcModel.predict(X_test)\n",
    "acc = accuracy_score(y_test, pred) * 100\n",
    "print(\"\\nGeneralization score = \", acc, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Classification Method # 2 - Random Forest Classifier (Pyspark)\n",
    "\n",
    "**Random Forest Classifier Scalability**\n",
    "\n",
    "The scalability for the current implementation of a random forest classifier is not very straightforward. However, one of the biggest bottlenecks lies in the fact that the decision trees are fully grown in a random forest. However, there have been implementations of this classifier through older versions of spark that were specifically made to be scalable.\n",
    "\n",
    "In one presentation, a scalable implementation of the random forest classifier is as follows (each part explained up to the point where the algorithm presents its enhancement):\n",
    "\n",
    "**Building Trees**\n",
    "\n",
    "1) Build multiple decision trees in the driver node\n",
    "\n",
    "2) Every time a new tree is built, an executor node will collect certain partition statistics used to generate future splits\n",
    "\n",
    "3) The executors then discretizes features (binning), which will be useful for aggregation back to the driver node later.\n",
    "\n",
    "**Training Trees**\n",
    "\n",
    "1) Set a certain threshold for child tree size\n",
    "\n",
    "2) Nodes get trained breadth first\n",
    "\n",
    "3) The Driver node will aggregate the partition statistics from the executors in order to generate efficient splits.\n",
    "\n",
    "4) If the size of a child tree for a split doesn't exceeds the aforementioned threshold, then train locally.\n",
    "\n",
    "Source: https://spark-summit.org/2014/wp-content/uploads/2014/07/Sequoia-Forest-Random-Forest-of-Humongous-Trees-Sung-Chung.pdf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----+--------------------+\n",
      "|predictedLabel|label|            features|\n",
      "+--------------+-----+--------------------+\n",
      "|            20|   20|[-0.1617844998836...|\n",
      "|            28|   28|[-0.0939981415867...|\n",
      "|            20|    3|[0.04951822757720...|\n",
      "|            21|   21|[-0.1100569367408...|\n",
      "|             9|    9|[0.28998464345932...|\n",
      "|             8|    8|[0.10590159893035...|\n",
      "|            32|   32|[-0.1558575183153...|\n",
      "|             9|    9|[-0.1934410631656...|\n",
      "|            26|   26|[-0.0818718224763...|\n",
      "|            16|   12|[-0.1234745830297...|\n",
      "+--------------+-----+--------------------+\n",
      "only showing top 10 rows\n",
      "\n",
      "Test Error = 0.475\n",
      "Generalized accuracy on test set: 0.525\n",
      "RandomForestClassificationModel (uid=rfc_00550df6e046) with 25 trees\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.feature import IndexToString, StringIndexer, VectorIndexer\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "#Create vectors from the numpy array containing the Olivetti Faces data\n",
    "vecs_train = []\n",
    "vecs_test = []\n",
    "\n",
    "for item in X_train:\n",
    "    vecs_train.append(Vectors.dense(item))\n",
    "    \n",
    "for item in X_test:\n",
    "    vecs_test.append(Vectors.dense(item))\n",
    "\n",
    "#Create Dictionary for denseVector features and their corresponding labels\n",
    "X_train_dict = dict(features= vecs_train, label=y_train)\n",
    "X_test_dict = dict(features=vecs_test, label=y_test)\n",
    "\n",
    "#Create Pandas DataFrame from the dictionary\n",
    "X_train_pdf = pd.DataFrame(X_train_dict)\n",
    "X_test_pdf = pd.DataFrame(X_test_dict)\n",
    "\n",
    "#Create training/test Pyspark DataFrame from pandas --- ERROR IN CREATION\n",
    "X_train_df = sqlContext.createDataFrame(X_train_pdf)\n",
    "X_test_df = sqlContext.createDataFrame(X_test_pdf)\n",
    "\n",
    "#Typecast Bigint type to smallint to speed up furhter computation.\n",
    "X_train_df = X_train_df.withColumn(\"label\", X_train_df[\"label\"].cast(\"smallint\"))\n",
    "X_test_df = X_test_df.withColumn(\"label\", X_test_df[\"label\"].cast(\"smallint\"))\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer(inputCol=\"label\", outputCol=\"indexedLabel\").fit(X_train_df)\n",
    "\n",
    "# Automatically identify categorical features, and index them.\n",
    "# Set maxCategories so features with > 4 distinct values are treated as continuous.\n",
    "featureIndexer =\\\n",
    "    VectorIndexer(inputCol=\"features\", outputCol=\"indexedFeatures\", maxCategories=4).fit(X_train_df)\n",
    "    \n",
    "# Train a RandomForest model.\n",
    "rf = RandomForestClassifier(labelCol=\"indexedLabel\", featuresCol=\"indexedFeatures\", numTrees=25)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString(inputCol=\"prediction\", outputCol=\"predictedLabel\",\n",
    "                               labels=labelIndexer.labels)\n",
    "\n",
    "# Chain indexers and forest in a Pipeline\n",
    "pipeline = Pipeline(stages=[labelIndexer, featureIndexer, rf, labelConverter])\n",
    "\n",
    "# Train model.  This also runs the indexers.\n",
    "model = pipeline.fit(X_train_df)\n",
    "\n",
    "# Make predictions.\n",
    "predictions = model.transform(X_test_df)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictions.select(\"predictedLabel\", \"label\", \"features\").show(10)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"indexedLabel\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracy))\n",
    "print(\"Generalized accuracy on test set:\", accuracy)\n",
    "\n",
    "rfModel = model.stages[2]\n",
    "print(rfModel)  # summary only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
